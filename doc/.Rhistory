})
lapply(test,function(x){
id <- which(!(x %in% unique(train[,colnames(x)])))
x[id] <- unique(train[,colnames(x)])[sample(1:nrow(unique(train[,colnames(x)])),1)]
})
lapply(test,function(x){
id <- which(!(x %in% unique(train[,colnames(x)])))
x[id] <- NA
#unique(train[,colnames(x)])[sample(1:nrow(unique(train[,colnames(x)])),1)]
})
test1=test[,which(unlist(lapply(test,function(x)
which(!(x %in% unique(train[,colnames(x)])))))),with=F]
test1=apply(test,c(1,2), function(x)
which(!(x %in% unique(train[,colnames(x)]))))
View(test1)
test1=apply(test,c(1,2), function(x)
!(x %in% unique(train[,colnames(x)])))
View(test1)
for(i in cat){
for(j in 1:nrow(test)){
t=unique(train[,i])
if(!test[j,i] %in% t){
test[j,i]=t[sample(1:length(t),1)]
}
}
}
unique(test$f5j4a)
f="gpa"
source("../lib/modelFunc.R")
data.filtered <- read.csv('../data/NAreplaced.csv') #4242 1388
select <- read.csv(paste0('../data/Updated_Features/',f,'_features.csv'),stringsAsFactors = FALSE)
data.filtered <- subset(data.filtered,select=c("challengeID",select$Codes)) # 4242*64
label <- read.csv('../data/train.csv')
label<-na.omit(label)
label<-subset(label,select=c("challengeID",f))
Index<-data.filtered$challengeID %in% label$challengeID
data.train<-data.filtered[Index,]
data.train<-as.data.frame(data.train)
data.train<-cbind(label[,-1], data.train[,-1])
colnames(data.train)[1]<-f
load("../data/categorical.RData")
cat=select$Codes[select$Codes %in% categorical]
data.train[,cat]=lapply(data.train[,cat],factor)
# create training and test data set
#set.seed(123)
train.index <- sample(1:nrow(data.train),800,replace = F)
train <- data.train[train.index,] #800*64
test <- data.train[-train.index,] #214*64
for(i in cat){
for(j in 1:nrow(test)){
t=unique(train[,i])
if(!test[j,i] %in% t){
test[j,i]=t[sample(1:length(t),1)]
}
}
}
y<-train[,1]
model_selection_con(train[,-1], test, y)
View(test)
load("../data/categorical.RData")
cat=select$Codes[select$Codes %in% categorical]
data.train[,cat]=lapply(data.train[,cat],factor)
# create training and test data set
#set.seed(123)
train.index <- sample(1:nrow(data.train),800,replace = F)
train <- data.train[train.index,] #800*64
test <- data.train[-train.index,] #214*64
for(i in cat){
for(j in 1:nrow(test)){
t=unique(train[,i])
if(!test[j,i] %in% t){
test[j,i]=t[sample(1:length(t),1)]
}
}
}
model_selection_con(train[,-1], test, y)
select <- read.csv('../data/Updated_Features/gpa_features.csv',stringsAsFactors = FALSE)
head(select)
as.data.table(select)
as.data.table(select)
data.table(select)
data.table(select)
as.data.table(select)
library(data.table)
as.data.table(select)
as.data.table(c(select$Codes,select$Description))
as.data.table(select)
source("../lib/modelFunc.R")
load("../data/categorical.RData")
data.filtered <- read.csv('../data/NAreplaced.csv') #4242 1388
select <- read.csv('../data/Updated_Features/gpa_features.csv')
select.idx<-colnames(data.filtered) %in% as.character(select$Codes)
data.filtered <- data.filtered[,select.idx]
label <- read.csv('../data/train.csv')
label<-label[!is.na(label$gpa),]
Index<-as.numeric(rownames(data.filtered))%in% label$challengeID
data.train<-data.filtered[Index,]
data.train<-as.data.frame(data.train)
cat.idx<-colnames(data.train) %in% categorical
for(i in which(cat.idx)){
data.train[,i]<-sapply(data.train[,i], factor)
test[,i]<-sapply(test[,i], factor)
id <- which(!(test[,i] %in% unique(data.train[,i])))
test[,i][id]<-sample(unique(data.train[,i]),length(id), replace = TRUE)
}
data.train<-cbind(label$gpa, data.train)
colnames(data.train)[1]<-"gpa"
# create training and test data set
set.seed(123)
train.index <- sample(1:nrow(data.train),800,replace = F)
train <- data.train[train.index,] #800*64
test <- data.train[-train.index,] #214*64
y<-train[,1]
model_selection_con(train[,-1], test, y)
f="gpa"
source("../lib/modelFunc.R")
data.filtered <- read.csv('../data/NAreplaced.csv') #4242 1388
select <- read.csv(paste0('../data/Updated_Features/',f,'_features.csv'),stringsAsFactors = FALSE)
data.filtered <- subset(data.filtered,select=c("challengeID",select$Codes)) # 4242*64
label <- read.csv('../data/train.csv')
label<-na.omit(label)
label<-subset(label,select=c("challengeID",f))
Index<-data.filtered$challengeID %in% label$challengeID
data.train<-data.filtered[Index,]
data.train<-as.data.frame(data.train)
data.train<-cbind(label[,-1], data.train[,-1])
colnames(data.train)[1]<-f
load("../data/categorical.RData")
cat=select$Codes[select$Codes %in% categorical]
data.train[,cat]=lapply(data.train[,cat],factor)
# create training and test data set
#set.seed(123)
train.index <- sample(1:nrow(data.train),800,replace = F)
train <- data.train[train.index,] #800*64
test <- data.train[-train.index,] #214*64
for(i in cat){
for(j in 1:nrow(test)){
t=unique(train[,i])
if(!test[j,i] %in% t){
test[j,i]=t[sample(1:length(t),1)]
}
}
}
y<-train[,1]
model_selection_con(train[,-1], test, y)
View(test)
f="gpa"
source("../lib/modelFunc.R")
data.filtered <- read.csv('../data/NAreplaced.csv') #4242 1388
select <- read.csv(paste0('../data/Updated_Features/',f,'_features.csv'),stringsAsFactors = FALSE)
data.filtered <- subset(data.filtered,select=c("challengeID",select$Codes)) # 4242*64
label <- read.csv('../data/train.csv')
label<-na.omit(label)
label<-subset(label,select=c("challengeID",f))
Index<-data.filtered$challengeID %in% label$challengeID
data.train<-data.filtered[Index,]
data.train<-as.data.frame(data.train)
data.train<-cbind(label[,-1], data.train[,-1])
colnames(data.train)[1]<-f
load("../data/categorical.RData")
cat=select$Codes[select$Codes %in% categorical]
data.train[,cat]=lapply(data.train[,cat],factor)
# create training and test data set
#set.seed(123)
train.index <- sample(1:nrow(data.train),800,replace = F)
train <- data.train[train.index,] #800*64
test <- data.train[-train.index,] #214*64
for(i in cat){
for(j in 1:nrow(test)){
t=unique(train[,i])
if(!test[j,i] %in% t){
test[j,i]=t[sample(1:length(t),1)]
}
}
}
y<-train[,1]
model_selection_con(train[,-1], test, y)
warnings()
f="gpa"
source("../lib/modelFunc.R")
data.filtered <- read.csv('../data/NAreplaced.csv') #4242 1388
select <- read.csv(paste0('../data/Updated_Features/',f,'_features.csv'),stringsAsFactors = FALSE)
data.filtered <- subset(data.filtered,select=c("challengeID",select$Codes)) # 4242*64
label <- read.csv('../data/train.csv')
label<-na.omit(label)
label<-subset(label,select=c("challengeID",f))
Index<-data.filtered$challengeID %in% label$challengeID
data.train<-data.filtered[Index,]
data.train<-as.data.frame(data.train)
data.train<-cbind(label[,-1], data.train[,-1])
colnames(data.train)[1]<-f
load("../data/categorical.RData")
cat=select$Codes[select$Codes %in% categorical]
data.train[,cat]=lapply(data.train[,cat],factor)
# create training and test data set
#set.seed(123)
train.index <- sample(1:nrow(data.train),800,replace = F)
train <- data.train[train.index,] #800*64
test <- data.train[-train.index,] #214*64
for(i in cat){
for(j in 1:nrow(test)){
t=unique(train[,i])
if(!test[j,i] %in% t){
test[j,i]=t[sample(1:length(t),1)]
}
}
}
y<-train[,1]
model_selection_con(train[,-1], test, y)
View(data.train)
View(train)
f="gpa"
source("../lib/helper_modelFunc.R")
data.filtered <- read.csv('../data/NAreplaced.csv') #4242 1388
select <- read.csv(paste0('../data/Updated_Features/',f,'_features.csv'),stringsAsFactors = FALSE)
data.filtered <- subset(data.filtered,select=c("challengeID",select$Codes)) # 4242*64
label <- read.csv('../data/train.csv')
label<-na.omit(label)
label<-subset(label,select=c("challengeID",f))
Index<-data.filtered$challengeID %in% label$challengeID
data.train<-data.filtered[Index,]
data.train<-as.data.frame(data.train)
data.train<-cbind(label[,-1], data.train[,-1])
colnames(data.train)[1]<-f
model_selection_con(data.train)
train.index <- sample(1:nrow(data.train),800,replace = F)
train <- data.train[train.index,] #800*64
test <- data.train[-train.index,] #214*64
y<-train[,1]
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "RMSE"
# Linear Regression
lin.reg <- lm(y~.,data = train)
test.pred.lin <- predict(lin.reg,test[,-1])
linErr<-Error(test, test.pred.lin)
View(data.train)
train.index <- sample(1:nrow(data.train),800,replace = F)
train <- data.train[train.index,] #800*64
test <- data.train[-train.index,] #214*64
y<-train[,1]
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "RMSE"
# Linear Regression
lin.reg <- lm(y~.,data = train[,-1])
test.pred.lin <- predict(lin.reg,test[,-1])
linErr<-Error(test, test.pred.lin)
load("../data/categorical.RData")
cat=select$Codes[select$Codes %in% categorical]
data.train[,cat]=lapply(data.train[,cat],factor)
# create training and test data set
#set.seed(123)
train.index <- sample(1:nrow(data.train),800,replace = F)
train <- data.train[train.index,] #800*64
test <- data.train[-train.index,] #214*64
for(i in cat){
for(j in 1:nrow(test)){
t=unique(train[,i])
if(!test[j,i] %in% t){
test[j,i]=t[sample(1:length(t),1)]
}
}
}
y=train[,1]
rt <- rpart(y~., data=train[,-1], method = "anova")
test.pred.rtree <- predict(rt,test[,-1])
treeErr<-Error(test, test.pred.rtree)
min.xerror <- rt$cptable[which.min(rt$cptable[,"xerror"]),"CP"]
rt.pruned <- prune(rt,cp = min.xerror)
test.pred.rtree.p <- predict(rt.pruned,test[,-1])
pruneErr<-Error(test, test.pred.rtree.p)
ct<-ctree(y~., data = train[,-1])
test.ct<-predict(ct, test[,-1])
ctErr<-Error(test, test.ct)
rf <- train(x=train[,-1], y=y, method="rf", trControl=control, metric=metric, verbose=FALSE)
test.rf<-predict(rf, test[,-1])
rfErr<-Error(test, test.rf)
View(train)
length_divisor<-6
iterations<-5000
predictions<-foreach(m=1:iterations,.combine=cbind) %do% {
training_positions <- sample(nrow(train), size=floor((nrow(train)/length_divisor)))
train_pos<-1:nrow(train) %in% training_positions
lm_fit<-lm(y[train_pos]~.,data=train[train_pos,])
predict(lm_fit,newdata=test[,-1])
}
lm_predictions<-rowMeans(predictions)
lmbagErr<-Error(test, lm_predictions)
#Ensemble Linear Regression and Random Forest
predictions<-(lm_predictions+test.rf)/2
lmrfErr<-Error(test, predictions)
length_divisor<-6
iterations<-5000
predictions<-foreach(m=1:iterations,.combine=cbind) %do% {
training_positions <- sample(nrow(train), size=floor((nrow(train)/length_divisor)))
train_pos<-1:nrow(train) %in% training_positions
lm_fit<-lm(y[train_pos]~.,data=train[train_pos,-1])
predict(lm_fit,newdata=test[,-1])
}
lm_predictions<-rowMeans(predictions)
lmbagErr<-Error(test, lm_predictions)
#Ensemble Linear Regression and Random Forest
predictions<-(lm_predictions+test.rf)/2
lmrfErr<-Error(test, predictions)
fit.gbm <- train(x=train, y=y, method="gbm", metric=metric, trControl=control, verbose=FALSE)
set.seed(seed)
fit.gbm <- train(x=train[,-1], y=y, method="gbm", metric=metric, trControl=control, verbose=FALSE)
gbm_prediction<-predict(fit.gbm, test[,-1])
gbmErr<-Error(test, gbm_prediction)
lin.reg <- lm(y~.,data = train[,-1])
test.pred.lin <- predict(lin.reg,test[,-1])
linErr<-Error(test, test.pred.lin)
label <- read.csv('../data/train.csv')
View(label)
train.index <- sample(1:nrow(data.train),800,replace = F)
train <- data.train[train.index,] #800*64
test <- data.train[-train.index,] #214*64
y<-factor(train[,1])
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "Accuracy"
set.seed(seed)
# Linear Regression
lin.reg <- train(y=y, x=train[,-1], method="glm", metric=metric, trControl=control)
test.pred.lin <- predict(lin.reg,test[,-1])
linErr<-Error2(test, test.pred.lin)
f="eviction"
source("../lib/helper_modelFunc.R")
data.filtered <- read.csv('../data/NAreplaced.csv') #4242 1388
select <- read.csv(paste0('../data/Updated_Features/',f,'_features.csv'),stringsAsFactors = FALSE)
data.filtered <- subset(data.filtered,select=c("challengeID",select$Codes)) # 4242*64
label <- read.csv('../data/train.csv')
label<-na.omit(label)
label<-subset(label,select=c("challengeID",f))
Index<-data.filtered$challengeID %in% label$challengeID
data.train<-data.filtered[Index,]
data.train<-as.data.frame(data.train)
data.train<-cbind(label[,-1], data.train[,-1])
colnames(data.train)[1]<-f
train.index <- sample(1:nrow(data.train),800,replace = F)
train <- data.train[train.index,] #800*64
test <- data.train[-train.index,] #214*64
y<-factor(train[,1])
control <- trainControl(method="repeatedcv", number=10, repeats=3)
seed <- 7
metric <- "Accuracy"
set.seed(seed)
# Linear Regression
lin.reg <- train(y=y, x=train[,-1], method="glm", metric=metric, trControl=control)
test.pred.lin <- predict(lin.reg,test[,-1])
linErr<-Error2(test, test.pred.lin)
rt <- rpart(y~., data=train[,-1], method = "class")
test.pred.rtree <- ifelse(predict(rt,test[,-1])[,1]>=predict(rt,test[,-1])[,2], 0, 1)
treeErr<-Error2(test, test.pred.rtree)
min.xerror <- rt$cptable[which.min(rt$cptable[,"xerror"]),"CP"]
rt.pruned <- prune(rt,cp = min.xerror)
test.pred.rtree.p <- ifelse(predict(rt.pruned,test[,-1])[,1]>=predict(rt.pruned,test[,-1])[,2], 0, 1)
pruneErr<-Error2(test, test.pred.rtree.p)
ct<-ctree(y~., data = train[,-1])
test.ct<-predict(ct, test[,-1])
ctErr<-Error2(test, test.ct)
rf <- train(x=train[,-1], y=y, method="rf", trControl=control, metric=metric, verbose=FALSE)
test.rf<-predict(rf, test[,-1])
rfErr<-Error2(test, test.rf)
svm_fit <- train(x=train[,-1], y=y, method="svmRadial", metric=metric, trControl=control, verbose=FALSE)
svm_predictions<-predict(svm_fit,newdata=test[,-1])
svmErr<-Error2(test, svm_predictions)
# Stochastic Gradient Boosting
fit.gbm <- train(x=train[,-1], y=y, method="gbm", metric=metric, trControl=control, verbose=FALSE)
gbm_prediction<-predict(fit.gbm, test[,-1])
gbmErr<-Error2(test, gbm_prediction)
# C5.0
fit.c50 <- train(x=train[,-1], y=y, method="C5.0", metric=metric, trControl=control)
library(C50)
fit.c50 <- train(x=train[,-1], y=y, method="C5.0", metric=metric, trControl=control)
c50_prediction<-predict(fit.c50 , test[,-1])
c50Err<-Error2(test, c50_prediction)
# lda
fit.lda <- train(x=train[,-1], y=factor(y), method="lda", metric=metric, trControl=control)
lda_prediction<-predict(fit.lda, test[,-1])
ldaErr<-Error2(test, lda_prediction)
# knn
fit.knn <- train(x=train[,-1], y=factor(y), method="knn", metric=metric, trControl=control)
knn_prediction<-predict(fit.knn, test[,-1])
knnErr<-Error2(test, knn_prediction)
load("../data/categorical.RData")
cat=select$Codes[select$Codes %in% categorical]
data.train[,cat]=lapply(data.train[,cat],factor)
# create training and test data set
#set.seed(123)
train.index <- sample(1:nrow(data.train),800,replace = F)
train <- data.train[train.index,] #800*64
test <- data.train[-train.index,] #214*64
for(i in cat){
for(j in 1:nrow(test)){
t=unique(train[,i])
if(!test[j,i] %in% t){
test[j,i]=t[sample(1:length(t),1)]
}
}
}
y=factor(train[,1])
# rpart function applied to a numeric variable => regression tree
rt <- rpart(y~., data=train[,-1], method = "class")
test.pred.rtree <- ifelse(predict(rt,test[,-1])[,1]>=predict(rt,test[,-1])[,2], 0, 1)
treeErr<-Error2(test, test.pred.rtree)
min.xerror <- rt$cptable[which.min(rt$cptable[,"xerror"]),"CP"]
# use it to prune the tree
rt.pruned <- prune(rt,cp = min.xerror)
test.pred.rtree.p <- ifelse(predict(rt.pruned,test[,-1])[,1]>=predict(rt.pruned,test[,-1])[,2], 0, 1)
pruneErr<-Error2(test, test.pred.rtree.p)
# Conditional inference trees via party
ct<-ctree(y~., data = train[,-1])
test.ct<-predict(ct, test[,-1])
ctErr<-Error2(test, test.ct)
# Random Forest
rf <- train(x=train[,-1], y=y, method="rf", trControl=control, metric=metric, verbose=FALSE)
test.rf<-predict(rf, test[,-1])
rfErr<-Error2(test, test.rf)
# SVM
svm_fit <- train(x=train[,-1], y=y, method="svmRadial", metric=metric, trControl=control, verbose=FALSE)
svm_predictions<-predict(svm_fit,newdata=test[,-1])
svmErr<-Error2(test, svm_predictions)
# Stochastic Gradient Boosting
fit.gbm <- train(x=train[,-1], y=y, method="gbm", metric=metric, trControl=control, verbose=FALSE)
gbm_prediction<-predict(fit.gbm, test[,-1])
gbmErr<-Error2(test, gbm_prediction)
# C5.0
fit.c50 <- train(x=train[,-1], y=y, method="C5.0", metric=metric, trControl=control)
c50_prediction<-predict(fit.c50 , test[,-1])
c50Err<-Error2(test, c50_prediction)
# lda
fit.lda <- train(x=train[,-1], y=factor(y), method="lda", metric=metric, trControl=control)
lda_prediction<-predict(fit.lda, test[,-1])
ldaErr<-Error2(test, lda_prediction)
# knn
fit.knn <- train(x=train[,-1], y=factor(y), method="knn", metric=metric, trControl=control)
knn_prediction<-predict(fit.knn, test[,-1])
knnErr<-Error2(test, knn_prediction)
label <- read.csv('../data/train.csv')
View(label)
source("../doc/helper_func.R")
for(f in c("gpa","grit","materialHardship","eviction","layoff","jobTraining")){
get.error(f)
}
load("../data/categorical.RData")
source("../doc/helper_func.R")
for(f in c("gpa","grit","materialHardship","eviction","layoff","jobTraining")){
get.error(f)
}
source("../doc/helper_func.R")
for(f in c("gpa","grit","materialHardship","eviction","layoff","jobTraining")){
get.error(f)
}
cat=select$Codes[select$Codes %in% categorical]
data.train[,cat]=lapply(data.train[,cat],factor)
View(data.train)
source("../doc/helper_func.R")
for(f in c("gpa","grit","materialHardship")){
get.error(f)
}
data.train[,cat]=lapply(data.train[,cat],factor)
source("../doc/helper_func.R")
for(f in c("gpa","grit","materialHardship")){
get.error(f)
}
View(train)
source("../doc/helper_func.R")
for(f in c("gpa","grit","materialHardship")){
get.error(f)
}
source("../lib/helper_modelFunc.R")
data.filtered <- read.csv('../data/NAreplaced.csv') #4242 1388
select <- read.csv(paste0('../data/Updated_Features/',f,'_features.csv'),stringsAsFactors = FALSE)
data.filtered <- subset(data.filtered,select=c("challengeID",select$Codes)) # 4242*64
label <- read.csv('../data/train.csv')
label<-na.omit(label)
label<-subset(label,select=c("challengeID",f))
Index<-data.filtered$challengeID %in% label$challengeID
data.train<-data.filtered[Index,]
data.train<-as.data.frame(data.train)
data.train<-cbind(label[,-1], data.train[,-1])
colnames(data.train)[1]<-f
load("~/GitHub/Spr2017-proj5-grp3/data/categorical.RData")
cat=select$Codes[select$Codes %in% categorical]
data.train[,cat]=lapply(data.train[,cat],factor)
# create training and test data set
#set.seed(123)
train.index <- sample(1:nrow(data.train),800,replace = F)
train <- data.train[train.index,] #800*64
test <- data.train[-train.index,] #214*64
for(i in cat){
for(j in 1:nrow(test)){
t=unique(train[,i])
if(!test[j,i] %in% t){
test[j,i]=t[sample(1:length(t),1)]
}
}
}
if(f %in% c("gpa","grit","materialHardship")){
y=train[,1]
tep=model_selection_con(train,test,y)
}else{
y=factor(train[,1])
tep=model_selection_cat(train,test,y)
}
tep
source("../doc/helper_func.R")
for(f in c("gpa","grit","materialHardship")){
get.error(f)
}
