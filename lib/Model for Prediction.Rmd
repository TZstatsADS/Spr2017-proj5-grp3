---
title: "Model for Prediction"
author: "Mengchen li"
output: pdf_document
---

```{r}
setwd("~/Desktop/Spr2017-proj5-grp3/data/")
data.filtered <- read.csv('NAreplaced.csv')#4242 2271
train.prediction <- read.csv('train.csv')
gpa <- train.prediction$gpa
data.filtered <- cbind(gpa,data.filtered)
data.filtered <- data.filtered[,-2]
# select rows with gpa data
data.filtered <- subset(data.filtered,!is.na(data.filtered$gpa))# 2330*2271

# create training and test data set
set.seed(123)
train.index <- sample(1:nrow(data.filtered),2000,replace = F)
train <- data.filtered[train.index,] #2000*2271
test <- data.filtered[-train.index,] #330*2271
```


# Model 1: Multiple linear regression
```{r}
# train model
lin.reg <- lm(gpa~.,data = train)
summary(lin.reg) #Multiple R-squared:0.7906  
# test error
test.pred.lin <- predict(lin.reg,test)
RMSE.lin.reg <- sqrt(mean((test.pred.lin-test$gpa)^2))                                      
RMSE.lin.reg # 4.000996e+12

MAE.lin.reg <- mean(abs(test.pred.lin-test$gpa))
MAE.lin.reg # 945849288500
```
Conclusion: test error is so large! bad model!


# Model 2: decision trees
```{r}
library(rpart)
library(rattle)
library(rpart.plot)

# train model
# rpart function applied to a numeric variable => regression tree
rt <- rpart(gpa~., data=train)
test.pred.rtree <- predict(rt,test) 

# test error
RMSE.rtree <- sqrt(mean((test.pred.rtree-test$gpa)^2))
RMSE.rtree #0.738875

MAE.rtree <- mean(abs(test.pred.rtree-test$gpa))
MAE.rtree #0.6042347

# check cross-validation results
printcp(rt) #Root node error:886.75/2000 = 0.44338

# get the optimal
min.xerror <- rt$cptable[which.min(rt$cptable[,"xerror"]),"CP"]
min.xerror #0.03019199

# use it to prune the tree
rt.pruned <- prune(rt,cp = min.xerror) 

# plot the pruned tree
fancyRpartPlot(rt.pruned)

# evaluate the new pruned tree on the test set
test.pred.rtree.p <- predict(rt.pruned,test)
RMSE.rtree.pruned <- sqrt(mean((test.pred.rtree.p-test$gpa)^2))
RMSE.rtree.pruned #0.6600364 reduced!
MAE.rtree.pruned <- mean(abs(test.pred.rtree.p-test$gpa))
MAE.rtree.pruned #0.5394455 reduced !
```
Conclusion: Although the error of tree method is still large, it seems better than Multiple linear regression model.


# Model 3: gamboostLSS
```{r}
library('gamboostLSS')

# train model
lmLSS <- glmboostLSS(gpa~., data = train)
coef(lmLSS, off2int = TRUE)

# plot mu and sigma
par(mfrow = c(1, 2), mar = c(4, 4, 2, 5))
plot(lmLSS, off2int = TRUE)

# test error for mu
test.pred.gam<- predict(lmLSS,test)
RMSE.gam <- sqrt(mean((test.pred.gam[[1]]-test$gpa)^2))
RMSE.gam #0.6691187

MAE.gam <- mean(abs(test.pred.gam[[1]]-test$gpa))
MAE.gam #0.5473816
```
Conclusion: The error of gamboostLSS is very similar with tree method.


# Model Comparision
```{r}
# Create a data frame with the error metrics for each method
accuracy <- data.frame(Method = c("Linear Regression","Full tree","Pruned tree",'gamboostLSS'),
RMSE   = c(RMSE.lin.reg,RMSE.rtree,RMSE.rtree.pruned,RMSE.gam),
MAE    = c(MAE.lin.reg,MAE.rtree,MAE.rtree.pruned,MAE.gam))

# Round the values and print the table
accuracy$RMSE <- round(accuracy$RMSE,2)
accuracy$MAE <- round(accuracy$MAE,2) 
accuracy

# create a data frame with the predictions for each method
all.predictions <- data.frame(actual = test$gpa,
                              linear.regression = test.pred.lin,
                              full.tree = test.pred.rtree,
                              pruned.tree = test.pred.rtree.p,
                              gamboostLSS = test.pred.gam[[1]])

# first six observations
head(all.predictions)
```
Conclusion:
pruned tree seems relative better than other models although the errors of all models are large.

Suggestion: try to select feature in advance and try these models again.



